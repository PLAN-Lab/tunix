{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "79e6e5f1"
      },
      "source": [
        "# Fine-tuning a Visual Language Model (VLM) using DPO\n",
        "\n",
        "This notebook demonstrates how to fine-tune a Visual Language Model (VLM), specifically the Gemma 3-1B-it model, using the Direct Preference Optimization (DPO) algorithm.\n",
        "\n",
        "The key steps involved are:\n",
        "\n",
        "1.  **Setup and Installations**: Install necessary libraries and dependencies.\n",
        "2.  **Model Loading**: Load the pre-trained Gemma 3-1B-it model.\n",
        "3.  **LoRA Application**: Apply Low-Rank Adaptation (LoRA) to the model for efficient fine-tuning.\n",
        "4.  **Data Loading and Preprocessing**: Load the RLAIF-V dataset and preprocess it for VLM training, including handling images and tokenizing text.\n",
        "5.  **DPO Training**: Set up and run the DPO training loop to fine-tune the model based on preference data (chosen and rejected responses).\n",
        "6.  **Logging and Visualization**: Log training metrics and visualize the training progress.\n",
        "\n",
        "The goal is to train the VLM to better align with human preferences by optimizing directly on pairs of preferred and dispreferred responses."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bjhtP5lgJOSg"
      },
      "outputs": [],
      "source": [
        "!pip install -q kagglehub\n",
        "\n",
        "!pip install -q tensorflow\n",
        "!pip install -q tensorboardX\n",
        "!pip install -q grain\n",
        "!pip install -q git+https://github.com/google/tunix\n",
        "!pip install -q git+https://github.com/google/qwix\n",
        "\n",
        "!pip uninstall -q -y flax\n",
        "!pip install -q git+https://github.com/google/flax.git\n",
        "\n",
        "!pip install -q huggingface_hub\n",
        "!pip install -q datasets\n",
        "!pip3 install jaxtyping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TezTyGV-Kgpi"
      },
      "outputs": [],
      "source": [
        "import functools\n",
        "import gc\n",
        "import os\n",
        "from pprint import pprint\n",
        "import re\n",
        "import time\n",
        "\n",
        "from flax import nnx\n",
        "import grain\n",
        "import humanize\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import kagglehub\n",
        "import optax\n",
        "from orbax import checkpoint as ocp\n",
        "import qwix\n",
        "import tensorflow_datasets as tfds\n",
        "from tqdm.auto import tqdm\n",
        "import math\n",
        "from tunix.generate import tokenizer_adapter as tokenizer_lib\n",
        "\n",
        "from tunix.examples.data import translation_dataset as data_lib\n",
        "from tunix.generate import sampler as sampler_lib\n",
        "from tunix.generate.vlm_sampler import VLMSampler\n",
        "from tunix.models.gemma3 import params as params_lib\n",
        "from tunix.models.gemma3 import params_safetensors as params_safetensors_lib\n",
        "\n",
        "from tunix.sft import metrics_logger\n",
        "from datasets import load_dataset\n",
        "\n",
        "from huggingface_hub import snapshot_download\n",
        "from tunix.sft.dpo.dpo_trainer import _generate_ids_and_masks\n",
        "from tunix.models.gemma3 import model as gemma3_model_lib\n",
        "from datasets import concatenate_datasets\n",
        "\n",
        "import types, json, os\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from orbax import checkpoint as ocp\n",
        "import optax\n",
        "from tunix.sft.dpo.dpo_trainer import DpoTrainer, DpoTrainingConfig\n",
        "from flax.serialization import to_state_dict\n",
        "from datasets import load_dataset\n",
        "import numpy as np, jax.numpy as jnp\n",
        "from PIL import Image\n",
        "from tunix.generate.utils import preprocess_image\n",
        "from tunix.sft.dpo.dpo_trainer import TrainingInput"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p6TZeaJMKmgt"
      },
      "outputs": [],
      "source": [
        "GEMMA_TOKENIZER_PATH = \"gs://gemma-data/tokenizers/tokenizer_gemma3.model\"\n",
        "model_id = \"google/gemma-3-1b-it\"\n",
        "IMAGE_SIZE = 224\n",
        "# ====== Data ======\n",
        "TRAIN_DATA_DIR = \"./data/train\"\n",
        "TEST_DATA_DIR = \"./data/test\"\n",
        "TRAIN_FRACTION = 1.0\n",
        "\n",
        "INTERMEDIATE_CKPT_DIR = \"/content/intermediate_ckpt/\"\n",
        "# ====== LoRA ======\n",
        "RANK = 32\n",
        "ALPHA = 16.0\n",
        "\n",
        "# ====== Sharding ======\n",
        "MESH = [(1, 1), (\"fsdp\", \"tp\")]\n",
        "\n",
        "MAX_PROMPT_LENGTH = 256\n",
        "TOTAL_GENERATION_STEPS = 256\n",
        "TEMPERATURE = 0.7\n",
        "TOP_P = 1.0\n",
        "TOP_K = 50\n",
        "BETA = 0.1\n",
        "\n",
        "# === AdamW, warmup, cosine scheduler ===\n",
        "LEARNING_RATE = 5e-6\n",
        "B1 = 0.9\n",
        "B2 = 0.99\n",
        "WEIGHT_DECAY = 0.1\n",
        "\n",
        "# == Cosine decay with warmup scheduler ==\n",
        "# Linearly increase learning rate from 0. to 5e-6 in the first 10% training\n",
        "# steps, and then gradually decrease the learning rate to 0 using cosine\n",
        "# scheduler.\n",
        "EVAL_EVERY_N_STEPS = 500\n",
        "MAX_STEPS = 50000\n",
        "BATCH_SIZE = 8\n",
        "EPOCHS = 80\n",
        "\n",
        "WARMUP_STEPS = 0.1 * MAX_STEPS\n",
        "# == Grad clipping ==\n",
        "# Grad clipping to prevent large gradients. Found this\n",
        "# important to keep KL divergence in check.\n",
        "MAX_GRAD_NORM = 0.1\n",
        "\n",
        "# ====== Inference ======\n",
        "GENERATION_CONFIGS = {\n",
        "    # greedy search\n",
        "    \"greedy\": {\"temperature\": 1e-4, \"top_k\": 1, \"top_p\": 1.0},\n",
        "    # some randomness\n",
        "    \"standard\": {\"temperature\": 0.7, \"top_k\": 50, \"top_p\": 0.95},\n",
        "    # liberal\n",
        "    \"liberal\": {\"temperature\": 0.85, \"top_k\": 2000, \"top_p\": 1.0},\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lUbAc5WPKwYh"
      },
      "outputs": [],
      "source": [
        "!huggingface-cli login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zhZCozOTKnmZ"
      },
      "outputs": [],
      "source": [
        "ignore_patterns = [\n",
        "    \"*.pth\",  # Ignore PyTorch .pth weight files\n",
        "]\n",
        "print(f\"Downloading {model_id} from Hugging Face...\")\n",
        "local_model_path = snapshot_download(\n",
        "    repo_id=model_id, ignore_patterns=ignore_patterns\n",
        ")\n",
        "print(f\"Model successfully downloaded to: {local_model_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "McqkpUNbKnja"
      },
      "outputs": [],
      "source": [
        "MODEL_CP_PATH = local_model_path\n",
        "\n",
        "model_config = (\n",
        "    gemma3_model_lib.Gemma3Config.gemma3_1b()\n",
        ")  # pick correponding config based on model version\n",
        "MESH = [(1, 1), (\"fsdp\", \"tp\")]\n",
        "mesh = jax.make_mesh(*MESH)\n",
        "with mesh:\n",
        "  gemma3 = params_safetensors_lib.create_model_from_safe_tensors(\n",
        "      MODEL_CP_PATH, model_config, mesh\n",
        "  )\n",
        "  nnx.display(gemma3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wT6HC4URKnaQ"
      },
      "outputs": [],
      "source": [
        "gemma_tokenizer = tokenizer_lib.Tokenizer(tokenizer_path=GEMMA_TOKENIZER_PATH)\n",
        "\n",
        "vlm_sampler = VLMSampler(\n",
        "    transformer=gemma3,\n",
        "    tokenizer=gemma_tokenizer,\n",
        "    image_size=IMAGE_SIZE,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a-0swaMJPSPb"
      },
      "outputs": [],
      "source": [
        "def get_lora_model(base_model, mesh):\n",
        "  lora_provider = qwix.LoraProvider(\n",
        "      module_path=(\n",
        "          \".*q_einsum|.*kv_einsum|.*gate_proj|.*down_proj|.*up_proj|\"\n",
        "          \".*attn_vec_einsum\"\n",
        "      ),\n",
        "      rank=RANK,\n",
        "      alpha=ALPHA,\n",
        "  )\n",
        "\n",
        "  model_input = base_model.get_model_input()\n",
        "  lora_model = qwix.apply_lora_to_model(\n",
        "      base_model, lora_provider, **model_input\n",
        "  )\n",
        "\n",
        "  with mesh:\n",
        "    state = nnx.state(lora_model)\n",
        "    pspecs = nnx.get_partition_spec(state)\n",
        "    sharded_state = jax.lax.with_sharding_constraint(state, pspecs)\n",
        "    nnx.update(lora_model, sharded_state)\n",
        "\n",
        "  return lora_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tEH8AioSPSOR"
      },
      "outputs": [],
      "source": [
        "# Policy model\n",
        "lora_gemma = get_lora_model(gemma3, mesh=mesh)\n",
        "nnx.display(lora_gemma)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "efDhGwrtPVZi"
      },
      "outputs": [],
      "source": [
        "SPLIT = \"train[:5000]\"\n",
        "\n",
        "ds = load_dataset(\"openbmb/RLAIF-V-Dataset\", split=SPLIT)\n",
        "cols = [\"image\", \"question\", \"chosen\", \"rejected\"]\n",
        "ds = ds.remove_columns([c for c in ds.column_names if c not in cols])\n",
        "\n",
        "\n",
        "def _pick_one_image(img_field):\n",
        "  \"\"\"Return a single PIL.Image from the dataset's image field.\"\"\"\n",
        "  x = img_field\n",
        "  if isinstance(x, list):\n",
        "    if not x:  # empty list, skip later\n",
        "      return None\n",
        "    x = x[0]\n",
        "  if isinstance(x, Image.Image):\n",
        "    return x.convert(\"RGB\")\n",
        "  arr = np.array(x)\n",
        "  if arr.ndim == 3:\n",
        "    return Image.fromarray(arr).convert(\"RGB\")\n",
        "  return None\n",
        "\n",
        "\n",
        "def preprocess_item(ex):\n",
        "  img = _pick_one_image(ex[\"image\"])\n",
        "  if img is None:\n",
        "    return {\n",
        "        \"pixel_values\": None,\n",
        "        \"question\": ex[\"question\"],\n",
        "        \"chosen\": ex[\"chosen\"],\n",
        "        \"rejected\": ex[\"rejected\"],\n",
        "    }\n",
        "  arr = np.array(img, dtype=np.uint8)[None, ...]  # [1,H,W,3]\n",
        "  px = preprocess_image(jnp.asarray(arr), IMAGE_SIZE)  # [1,S,S,3] float32\n",
        "  return {\n",
        "      \"pixel_values\": np.asarray(px[0]),  # [S,S,3]\n",
        "      \"question\": ex[\"question\"],\n",
        "      \"chosen\": ex[\"chosen\"],\n",
        "      \"rejected\": ex[\"rejected\"],\n",
        "  }\n",
        "\n",
        "\n",
        "ds = ds.with_transform(preprocess_item)\n",
        "\n",
        "PAD = gemma_tokenizer.pad_id()\n",
        "EOS = gemma_tokenizer.eos_id()\n",
        "\n",
        "\n",
        "def _left_pad_np(ids, L, pad=PAD):\n",
        "  ids = ids[-L:] if len(ids) \u003e L else [pad] * (L - len(ids)) + ids\n",
        "  return np.asarray(ids, dtype=np.int32)\n",
        "\n",
        "\n",
        "def _right_pad_np(ids, L, pad=PAD):\n",
        "  ids = ids[:L]\n",
        "  ids = ids + [pad] * (L - len(ids))\n",
        "  return np.asarray(ids, dtype=np.int32)\n",
        "\n",
        "\n",
        "def _make_mask(ids, pad=PAD):\n",
        "  return (ids != pad).astype(np.int32)\n",
        "\n",
        "\n",
        "def numpy_batches_vlm(dataset, batch_size=1, shuffle=True, seed=0, epochs=None):\n",
        "  rng = np.random.default_rng(seed)\n",
        "  epoch = 0\n",
        "  while True:\n",
        "    idx = np.arange(len(dataset))\n",
        "    if shuffle:\n",
        "      rng.shuffle(idx)\n",
        "\n",
        "    buf = []\n",
        "    for i in idx:\n",
        "      ex = dataset[int(i)]\n",
        "      if ex[\"pixel_values\"] is None:\n",
        "        continue\n",
        "      buf.append(ex)\n",
        "      if len(buf) == batch_size:\n",
        "        qs = [b[\"question\"] for b in buf]\n",
        "        chs = [b[\"chosen\"] for b in buf]\n",
        "        rjs = [b[\"rejected\"] for b in buf]\n",
        "\n",
        "        q_tok = [gemma_tokenizer.encode(x) for x in qs]\n",
        "        ch_tok = [gemma_tokenizer.encode(x) + [EOS] for x in chs]\n",
        "        rj_tok = [gemma_tokenizer.encode(x) + [EOS] for x in rjs]\n",
        "\n",
        "        Q = np.stack(\n",
        "            [_left_pad_np(ids, MAX_PROMPT_LENGTH) for ids in q_tok], axis=0\n",
        "        )\n",
        "        CH = np.stack(\n",
        "            [_right_pad_np(ids, TOTAL_GENERATION_STEPS) for ids in ch_tok],\n",
        "            axis=0,\n",
        "        )\n",
        "        RJ = np.stack(\n",
        "            [_right_pad_np(ids, TOTAL_GENERATION_STEPS) for ids in rj_tok],\n",
        "            axis=0,\n",
        "        )\n",
        "        PX = np.stack([b[\"pixel_values\"] for b in buf], axis=0).astype(\n",
        "            np.float32\n",
        "        )\n",
        "\n",
        "        Q_mask = np.stack([_make_mask(ids, PAD) for ids in Q], axis=0)\n",
        "        CH_mask = np.stack([_make_mask(ids, PAD) for ids in CH], axis=0)\n",
        "        RJ_mask = np.stack([_make_mask(ids, PAD) for ids in RJ], axis=0)\n",
        "\n",
        "        yield TrainingInput(\n",
        "            prompt_ids=jnp.asarray(Q),\n",
        "            prompt_mask=jnp.asarray(Q_mask),\n",
        "            chosen_ids=jnp.asarray(CH),\n",
        "            chosen_mask=jnp.asarray(CH_mask),\n",
        "            rejected_ids=jnp.asarray(RJ),\n",
        "            rejected_mask=jnp.asarray(RJ_mask),\n",
        "            pixel_values=jnp.asarray(PX),\n",
        "        )\n",
        "        buf = []\n",
        "\n",
        "    if buf:\n",
        "      qs = [b[\"question\"] for b in buf]\n",
        "      chs = [b[\"chosen\"] for b in buf]\n",
        "      rjs = [b[\"rejected\"] for b in buf]\n",
        "      q_tok = [gemma_tokenizer.encode(x) for x in qs]\n",
        "      ch_tok = [gemma_tokenizer.encode(x) + [EOS] for x in chs]\n",
        "      rj_tok = [gemma_tokenizer.encode(x) + [EOS] for x in rjs]\n",
        "      Q = np.stack(\n",
        "          [_left_pad_np(ids, MAX_PROMPT_LENGTH) for ids in q_tok], axis=0\n",
        "      )\n",
        "      CH = np.stack(\n",
        "          [_right_pad_np(ids, TOTAL_GENERATION_STEPS) for ids in ch_tok], axis=0\n",
        "      )\n",
        "      RJ = np.stack(\n",
        "          [_right_pad_np(ids, TOTAL_GENERATION_STEPS) for ids in rj_tok], axis=0\n",
        "      )\n",
        "      PX = np.stack([b[\"pixel_values\"] for b in buf], axis=0).astype(np.float32)\n",
        "      Q_mask = np.stack([_make_mask(ids, PAD) for ids in Q], axis=0)\n",
        "      CH_mask = np.stack([_make_mask(ids, PAD) for ids in CH], axis=0)\n",
        "      RJ_mask = np.stack([_make_mask(ids, PAD) for ids in RJ], axis=0)\n",
        "      yield TrainingInput(\n",
        "          prompt_ids=jnp.asarray(Q),\n",
        "          prompt_mask=jnp.asarray(Q_mask),\n",
        "          chosen_ids=jnp.asarray(CH),\n",
        "          chosen_mask=jnp.asarray(CH_mask),\n",
        "          rejected_ids=jnp.asarray(RJ),\n",
        "          rejected_mask=jnp.asarray(RJ_mask),\n",
        "          pixel_values=jnp.asarray(PX),\n",
        "      )\n",
        "\n",
        "    epoch += 1\n",
        "    if epochs is not None and epoch \u003e= epochs:\n",
        "      break\n",
        "\n",
        "\n",
        "# Smoke one batch\n",
        "\n",
        "b0 = next(numpy_batches_vlm(ds, batch_size=4))\n",
        "print(\"Batch pixels:\", b0.pixel_values.shape, \"| B:\", b0.prompt_ids.shape[0])\n",
        "print(\"Batch prompt_ids:\", b0.prompt_ids.shape)\n",
        "print(\"Batch prompt_mask:\", b0.prompt_mask.shape)\n",
        "print(\"Batch chosen_ids:\", b0.chosen_ids.shape)\n",
        "print(\"Batch chosen_mask:\", b0.chosen_mask.shape)\n",
        "print(\"Batch rejected_ids:\", b0.rejected_ids.shape)\n",
        "print(\"Batch rejected_mask:\", b0.rejected_mask.shape)\n",
        "\n",
        "print(\"Dataset size:\", len(ds))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Krvg-IdWPSNQ"
      },
      "outputs": [],
      "source": [
        "# --- Eval helpers (no grads) ---\n",
        "\n",
        "\n",
        "def _make_pos_and_causal_mask(tokens: jnp.ndarray, pad_id: int):\n",
        "  valid = tokens != pad_id\n",
        "  positions = (jnp.cumsum(valid.astype(jnp.int32), axis=1) - 1) * valid.astype(\n",
        "      jnp.int32\n",
        "  )\n",
        "  L = tokens.shape[1]\n",
        "  causal = jnp.tril(jnp.ones((L, L), dtype=bool))\n",
        "  attn_mask = valid[..., None] \u0026 valid[:, None, :] \u0026 causal[None, ...]\n",
        "  return positions.astype(jnp.int32), attn_mask\n",
        "\n",
        "\n",
        "def _vlm_forward_and_cache(\n",
        "    model_mod, *, tokens: jnp.ndarray, pixel_values: jnp.ndarray, pad_id: int\n",
        "):\n",
        "  positions, attn_mask = _make_pos_and_causal_mask(tokens, pad_id)\n",
        "  logits, _cache_out = model_mod(\n",
        "      last_tokens=tokens.astype(jnp.int32),\n",
        "      positions=positions,\n",
        "      cache={},  # fresh cache per call\n",
        "      attention_mask=attn_mask,\n",
        "      pixel_values=pixel_values.astype(jnp.float32),\n",
        "      output_hidden_states=False,\n",
        "  )\n",
        "  return logits\n",
        "\n",
        "\n",
        "def _seq_logprob_answer(model_mod, px, prompt_ids, answer_ids, pad_id: int):\n",
        "  # teacher-forced: concat prompt with shifted answer\n",
        "  x_ids = jnp.concatenate([prompt_ids, answer_ids[:, :-1]], axis=1)\n",
        "  logits = _vlm_forward_and_cache(\n",
        "      model_mod, tokens=x_ids, pixel_values=px, pad_id=pad_id\n",
        "  )\n",
        "  La = answer_ids.shape[1]\n",
        "  ans_logits = logits[:, -La:, :]\n",
        "  logp = jax.nn.log_softmax(ans_logits, axis=-1)\n",
        "  tok_logp = jnp.take_along_axis(logp, answer_ids[..., None], axis=-1)[..., 0]\n",
        "  ans_mask = (answer_ids != pad_id).astype(tok_logp.dtype)\n",
        "  return (tok_logp * ans_mask).sum(axis=1)  # [B]\n",
        "\n",
        "\n",
        "def _eval_batch_metrics(policy_mod, ref_mod, batch, pad_id: int, beta: float):\n",
        "  px = jnp.asarray(batch.pixel_values, dtype=jnp.float32)\n",
        "  q = jnp.asarray(batch.prompt_ids, dtype=jnp.int32)\n",
        "  ch = jnp.asarray(batch.chosen_ids, dtype=jnp.int32)\n",
        "  rj = jnp.asarray(batch.rejected_ids, dtype=jnp.int32)\n",
        "\n",
        "  lp_ch = _seq_logprob_answer(policy_mod, px, q, ch, pad_id)\n",
        "  lp_rj = _seq_logprob_answer(policy_mod, px, q, rj, pad_id)\n",
        "  lq_ch = _seq_logprob_answer(ref_mod, px, q, ch, pad_id)\n",
        "  lq_rj = _seq_logprob_answer(ref_mod, px, q, rj, pad_id)\n",
        "\n",
        "  pol_margin = lp_ch - lp_rj  # [B]\n",
        "  ref_margin = lq_ch - lq_rj  # [B]\n",
        "  adv = pol_margin - ref_margin  # [B]\n",
        "  # DPO loss (forward-only)\n",
        "  loss = -jax.nn.log_sigmoid(beta * adv).mean()\n",
        "\n",
        "  acc = (pol_margin \u003e 0).mean()\n",
        "  return {\n",
        "      \"loss\": float(loss),\n",
        "      \"acc\": float(acc),\n",
        "      \"pol_margin_mean\": float(pol_margin.mean()),\n",
        "      \"ref_margin_mean\": float(ref_margin.mean()),\n",
        "      \"adv_mean\": float(adv.mean()),\n",
        "      \"num_correct\": float((pol_margin \u003e 0).sum()),\n",
        "      \"batch_size\": float(pol_margin.shape[0]),\n",
        "  }\n",
        "\n",
        "\n",
        "def _mean_ci(xs):\n",
        "  xs = [float(x) for x in xs]\n",
        "  if not xs:\n",
        "    return (float(\"nan\"), float(\"nan\"))\n",
        "  m = sum(xs) / len(xs)\n",
        "  var = sum((x - m) ** 2 for x in xs) / max(1, len(xs) - 1)\n",
        "  se = math.sqrt(var / max(1, len(xs)))\n",
        "  return m, 1.96 * se"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a_U74SgjPSE1"
      },
      "outputs": [],
      "source": [
        "def run_eval_vlm(\n",
        "    policy_mod,\n",
        "    ref_mod,\n",
        "    dataset,\n",
        "    *,\n",
        "    batches=50,\n",
        "    batch_size=16,\n",
        "    seed=7,\n",
        "    beta=BETA,\n",
        "    pad_id=PAD,\n",
        "):\n",
        "  itr = numpy_batches_vlm(\n",
        "      dataset, batch_size=batch_size, shuffle=True, seed=seed, epochs=1\n",
        "  )\n",
        "  losses, accs, pols, refs, advs = [], [], [], [], []\n",
        "  tot_corr = 0.0\n",
        "  tot_seen = 0.0\n",
        "\n",
        "  for i in range(batches):\n",
        "    try:\n",
        "      batch = next(itr)\n",
        "    except StopIteration:\n",
        "      break\n",
        "    out = _eval_batch_metrics(\n",
        "        policy_mod, ref_mod, batch, pad_id=pad_id, beta=beta\n",
        "    )\n",
        "    losses.append(out[\"loss\"])\n",
        "    accs.append(out[\"acc\"])\n",
        "    pols.append(out[\"pol_margin_mean\"])\n",
        "    refs.append(out[\"ref_margin_mean\"])\n",
        "    advs.append(out[\"adv_mean\"])\n",
        "    tot_corr += out[\"num_correct\"]\n",
        "    tot_seen += out[\"batch_size\"]\n",
        "\n",
        "  loss_m, loss_ci = _mean_ci(losses)\n",
        "  acc_m, acc_ci = _mean_ci(accs)\n",
        "  pol_m, pol_ci = _mean_ci(pols)\n",
        "  ref_m, ref_ci = _mean_ci(refs)\n",
        "  adv_m, adv_ci = _mean_ci(advs)\n",
        "\n",
        "  summary = {\n",
        "      \"batches_evaluated\": len(losses),\n",
        "      \"examples_evaluated\": int(tot_seen),\n",
        "      \"accuracy_mean\": acc_m,\n",
        "      \"accuracy_95ci\": acc_ci,\n",
        "      \"cumulative_accuracy\": tot_corr / max(1.0, tot_seen),\n",
        "      \"loss_mean\": loss_m,\n",
        "      \"loss_95ci\": loss_ci,\n",
        "      \"policy_margin_mean\": pol_m,\n",
        "      \"policy_margin_95ci\": pol_ci,\n",
        "      \"ref_margin_mean\": ref_m,\n",
        "      \"ref_margin_95ci\": ref_ci,\n",
        "      \"advantage_mean\": adv_m,\n",
        "      \"advantage_95ci\": adv_ci,\n",
        "      \"reward_margin_mean (beta*adv)\": beta * adv_m,\n",
        "  }\n",
        "  print(\"\\n=== DPO Eval (VLM) ===\")\n",
        "  print(\n",
        "      f\"batches={summary['batches_evaluated']} \"\n",
        "      f\" examples={summary['examples_evaluated']}\"\n",
        "  )\n",
        "  print(\n",
        "      f\"Accuracy (mean±95% CI): {acc_m:.3f} ± {acc_ci:.3f} | Cumulative:\"\n",
        "      f\" {summary['cumulative_accuracy']:.3f}\"\n",
        "  )\n",
        "  print(f\"Loss     (mean±95% CI): {loss_m:.4f} ± {loss_ci:.4f}\")\n",
        "  print(f\"Pol Δ    (mean±95% CI): {pol_m:+.3f} ± {pol_ci:.3f}\")\n",
        "  print(f\"Ref Δ    (mean±95% CI): {ref_m:+.3f} ± {ref_ci:.3f}\")\n",
        "  print(\n",
        "      f\"Adv      (mean±95% CI): {adv_m:+.3f} ± {adv_ci:.3f}  (reward ≈ β*adv)\"\n",
        "  )\n",
        "  return summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zRb1btGAPSBo"
      },
      "outputs": [],
      "source": [
        "# Make a held-out slice (different from SPLIT used for training)\n",
        "ds_eval = load_dataset(\"openbmb/RLAIF-V-Dataset\", split=\"train[5000:5400]\")\n",
        "ds_eval = ds_eval.remove_columns([\n",
        "    c\n",
        "    for c in ds_eval.column_names\n",
        "    if c not in [\"image\", \"question\", \"chosen\", \"rejected\"]\n",
        "])\n",
        "ds_eval = ds_eval.with_transform(preprocess_item)\n",
        "\n",
        "# with mesh:\n",
        "#     eval_summary = run_eval_vlm(lora_gemma, gemma3, ds_eval, batches=50, batch_size=16, seed=11)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XKNb6B_IT09R"
      },
      "outputs": [],
      "source": [
        "INTERMEDIATE_CKPT_DIR = \"/content/intermediate_ckpt_vlm\"\n",
        "os.makedirs(INTERMEDIATE_CKPT_DIR, exist_ok=True)\n",
        "HIST_PATH = os.path.join(INTERMEDIATE_CKPT_DIR, \"train_history.json\")\n",
        "\n",
        "HISTORY = {\n",
        "    \"step\": [],\n",
        "    \"loss\": [],\n",
        "    \"rewards/chosen\": [],\n",
        "    \"rewards/rejected\": [],\n",
        "    \"rewards/margin\": [],\n",
        "    \"rewards/accuracy\": [],\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GBv6XLfLT9sG"
      },
      "outputs": [],
      "source": [
        "config = DpoTrainingConfig(\n",
        "    eval_every_n_steps=EVAL_EVERY_N_STEPS,\n",
        "    max_steps=MAX_STEPS,\n",
        "    beta=BETA,\n",
        "    label_smoothing=0.0,\n",
        ")\n",
        "optimizer = optax.adamw(learning_rate=LEARNING_RATE)\n",
        "train_batches = numpy_batches_vlm(\n",
        "    ds, batch_size=BATCH_SIZE, shuffle=False, seed=42, epochs=EPOCHS\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7PDdv1t8Aom2"
      },
      "outputs": [],
      "source": [
        "with mesh:\n",
        "  trainer = DpoTrainer(\n",
        "      model=lora_gemma,\n",
        "      ref_model=gemma3,\n",
        "      optimizer=optimizer,\n",
        "      training_config=config,\n",
        "  )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jA6xdidxAl0m"
      },
      "outputs": [],
      "source": [
        "_orig_post = getattr(trainer, \"_post_process_train_step\", None)\n",
        "\n",
        "\n",
        "def _patched_post_process_train_step(self, aux):\n",
        "  if _orig_post is not None:\n",
        "    _orig_post(aux)\n",
        "\n",
        "  s = int(getattr(self, \"_train_steps\", 0))\n",
        "\n",
        "  loss_val = float(\"nan\")\n",
        "  bm = getattr(self, \"_buffered_train_metrics\", None)\n",
        "  if bm is not None and getattr(bm, \"losses\", None):\n",
        "    try:\n",
        "      loss_val = float(bm.losses[-1])\n",
        "    except Exception:\n",
        "      pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JnfCXXR_AqQa"
      },
      "outputs": [],
      "source": [
        "_orig_post = getattr(trainer, \"_post_process_train_step\", None)\n",
        "\n",
        "\n",
        "def _patched_post_process_train_step(self, aux):\n",
        "  if _orig_post is not None:\n",
        "    _orig_post(aux)\n",
        "\n",
        "  s = int(getattr(self, \"_train_steps\", 0))\n",
        "\n",
        "  loss_val = float(\"nan\")\n",
        "  bm = getattr(self, \"_buffered_train_metrics\", None)\n",
        "  if bm is not None and getattr(bm, \"losses\", None):\n",
        "    try:\n",
        "      loss_val = float(bm.losses[-1])\n",
        "    except Exception:\n",
        "      pass\n",
        "\n",
        "  HISTORY[\"step\"].append(s)\n",
        "  HISTORY[\"loss\"].append(loss_val)\n",
        "  HISTORY[\"rewards/chosen\"].append(float(aux[\"rewards/chosen\"]))\n",
        "  HISTORY[\"rewards/rejected\"].append(float(aux[\"rewards/rejected\"]))\n",
        "  HISTORY[\"rewards/margin\"].append(float(aux[\"rewards/margin\"]))\n",
        "  HISTORY[\"rewards/accuracy\"].append(float(aux[\"rewards/accuracy\"]))\n",
        "\n",
        "  if s % EVAL_EVERY_N_STEPS == 0:\n",
        "    print(\n",
        "        \"[metric]\"\n",
        "        f\" step={s} loss={loss_val:.4f} margin={float(aux['rewards/margin']):.4f}\"\n",
        "    )\n",
        "\n",
        "\n",
        "trainer._post_process_train_step = types.MethodType(\n",
        "    _patched_post_process_train_step, trainer\n",
        ")\n",
        "print(\"✅ metrics capture patched on current trainer\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dDdXwCutA9bB"
      },
      "outputs": [],
      "source": [
        "with mesh:\n",
        "  trainer.train(train_batches)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9B4qmjLxBH2B"
      },
      "outputs": [],
      "source": [
        "# optional: persist\n",
        "with open(HIST_PATH, \"w\") as f:\n",
        "  json.dump(HISTORY, f)\n",
        "print(\"📈 history saved to:\", HIST_PATH, \"| points:\", len(HISTORY[\"step\"]))\n",
        "\n",
        "\n",
        "# --- plotting ---\n",
        "def _safe_xy(hist, key):\n",
        "  x = np.array(hist.get(\"step\", []), dtype=float)\n",
        "  y = np.array(hist.get(key, []), dtype=float)\n",
        "  return x, y\n",
        "\n",
        "\n",
        "def moving_average(data, window_size):\n",
        "  \"\"\"Calculates the moving average of a list or numpy array.\"\"\"\n",
        "  if len(data) \u003c window_size:\n",
        "    return (\n",
        "        data  # Return original data if window size is larger than data length\n",
        "    )\n",
        "  return np.convolve(data, np.ones(window_size) / window_size, mode=\"valid\")\n",
        "\n",
        "\n",
        "def _plot_series(x, y, title, ylabel, window_size=5):\n",
        "  if len(x) == 0:\n",
        "    print(f\"[plot] no data for {title}\")\n",
        "    return\n",
        "  plt.figure()\n",
        "  # Apply moving average\n",
        "  y_smooth = moving_average(y, window_size)\n",
        "  x_smooth = x[\n",
        "      window_size - 1 :\n",
        "  ]  # Adjust x to match the length of the smoothed data\n",
        "  plt.plot(x_smooth, y_smooth)\n",
        "  plt.title(title)\n",
        "  plt.xlabel(\"step\")\n",
        "  plt.ylabel(ylabel)\n",
        "  plt.grid(True)\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "x, y = _safe_xy(HISTORY, \"loss\")\n",
        "_plot_series(x, y, \"Training Loss (Smoothed)\", \"loss\")\n",
        "\n",
        "x, y = _safe_xy(HISTORY, \"rewards/margin\")\n",
        "_plot_series(x, y, \"Rewards Margin (chosen - rejected) (Smoothed)\", \"margin\")\n",
        "\n",
        "x, ch = _safe_xy(HISTORY, \"rewards/chosen\")\n",
        "_, rj = _safe_xy(HISTORY, \"rewards/rejected\")\n",
        "if len(x):\n",
        "  plt.figure()\n",
        "  window_size = 10\n",
        "  ch_smooth = moving_average(ch, window_size)\n",
        "  rj_smooth = moving_average(rj, window_size)\n",
        "  x_smooth = x[window_size - 1 :]\n",
        "  plt.plot(x_smooth, ch_smooth, label=\"chosen (Smoothed)\")\n",
        "  plt.plot(x_smooth, rj_smooth, label=\"rejected (Smoothed)\")\n",
        "  plt.title(\"Chosen vs Rejected Rewards (Smoothed)\")\n",
        "  plt.xlabel(\"step\")\n",
        "  plt.ylabel(\"reward\")\n",
        "  plt.legend()\n",
        "  plt.grid(True)\n",
        "  plt.show()\n",
        "\n",
        "x, y = _safe_xy(HISTORY, \"rewards/accuracy\")\n",
        "_plot_series(x, y, \"Rewards Accuracy (Smoothed)\", \"accuracy\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V6E1",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
