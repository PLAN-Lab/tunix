{
  "cells": [
    {
      "metadata": {
        "id": "OeEzmwEz-Mls"
      },
      "cell_type": "markdown",
      "source": [
        "\u003ca href=\"https://colab.research.google.com/github/google/tunix/blob/main/examples/qwen3_example.ipynb\" \u003e\u003cimg src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/\u003e\u003c/a\u003e\n",
        "\n",
        "## Install necessary libraries"
      ]
    },
    {
      "metadata": {
        "id": "_C8Gg0naouTB"
      },
      "cell_type": "code",
      "source": [
        "!pip install -q kagglehub\n",
        "\n",
        "!pip install -q tensorflow\n",
        "!pip install -q tensorboardX\n",
        "!pip install -q grain\n",
        "!pip install -q git+https://github.com/google/tunix\n",
        "!pip install -q git+https://github.com/google/qwix\n",
        "\n",
        "!pip uninstall -q -y flax\n",
        "!pip install -q git+https://github.com/google/flax.git\n",
        "\n",
        "!pip install -q datasets"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "OO74m52JpJwp"
      },
      "cell_type": "markdown",
      "source": [
        "# Down the weights from Kaggle"
      ]
    },
    {
      "metadata": {
        "id": "6f-FF3V6b_y9"
      },
      "cell_type": "code",
      "source": [
        "from flax import nnx\n",
        "import kagglehub\n",
        "from tunix.models.qwen3 import model\n",
        "from tunix.models.qwen3 import params\n",
        "\n",
        "MODEL_CP_PATH = kagglehub.model_download(\"qwen-lm/qwen-3/transformers/0.6b\")\n",
        "\n",
        "config = (\n",
        "    model.ModelConfig.qwen3_0_6b()\n",
        ")  # pick correponding config based on model version\n",
        "qwen3 = params.create_model_from_safe_tensors(MODEL_CP_PATH, config)\n",
        "nnx.display(qwen3)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "bHoIcCFVfLgv"
      },
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_CP_PATH)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "YPxYtxz4pQ07"
      },
      "cell_type": "markdown",
      "source": [
        "# Run inference"
      ]
    },
    {
      "metadata": {
        "id": "IY_BGK_OfWuX"
      },
      "cell_type": "code",
      "source": [
        "from tunix.generate import sampler\n",
        "\n",
        "\n",
        "def templatize(prompts):\n",
        "  out = []\n",
        "  for p in prompts:\n",
        "    out.append(\n",
        "        tokenizer.apply_chat_template(\n",
        "            [\n",
        "                {\"role\": \"user\", \"content\": p},\n",
        "            ],\n",
        "            tokenize=False,\n",
        "            add_generation_prompt=True,\n",
        "            enable_thinking=True,\n",
        "        )\n",
        "    )\n",
        "  return out\n",
        "\n",
        "\n",
        "inputs = templatize([\n",
        "    \"which is larger 9.9 or 9.11?\",\n",
        "    \"如何制作月饼?\",\n",
        "    \"tell me your name, respond in Chinese\",\n",
        "])\n",
        "\n",
        "sampler = sampler.Sampler(\n",
        "    qwen3,\n",
        "    tokenizer,\n",
        "    sampler.CacheConfig(\n",
        "        cache_size=256, num_layers=28, num_kv_heads=8, head_dim=128\n",
        "    ),\n",
        ")\n",
        "out = sampler(inputs, max_generation_steps=128, echo=True)\n",
        "\n",
        "for t in out.text:\n",
        "  print(t)\n",
        "  print(\"*\" * 30)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "Vr_9ULDVA2-Y"
      },
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "colab": {
      "last_runtime": {
        "build_target": "",
        "kind": "local"
      },
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
